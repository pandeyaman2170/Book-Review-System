{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":63658,"databundleVersionId":6953956,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport tensorflow as tf\n\nphysical_devices = tf.config.list_physical_devices('GPU')\nif len(physical_devices) > 0:\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n\n        \nimport string\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer  # Add this import statement\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Load the dataset\ndf = pd.read_csv('/kaggle/input/ratemeter/train.csv')\ndf_subset = df.iloc[:600000]\n\n# Preprocess and vectorize the text\ndef preprocess_and_vectorize(text):\n    text = text.lower()\n    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n    text = ''.join(e for e in text if e.isalnum() or e.isspace())\n    tokens = word_tokenize(text)\n    stop_words = set(stopwords.words('english'))\n    tokens = [token for token in tokens if token not in stop_words]\n    porter = PorterStemmer()\n    tokens = [porter.stem(token) for token in tokens]\n    preprocessed_text = ' '.join(tokens)\n    return preprocessed_text\n\ndf_subset['review_text'] = df_subset['review_text'].apply(preprocess_and_vectorize)\n\nprint(\"preprocessing is done ......\")\n# Tokenize the text\nmax_words = 5000\ntokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\ntokenizer.fit_on_texts(df_subset['review_text'])\nX = tokenizer.texts_to_sequences(df_subset['review_text'])\nX = pad_sequences(X)\n\nprint(\"vectorization is done ......\")\n\n# Split the data into training and testing sets\ny = df_subset['rating']\nnum_classes = len(df_subset['rating'].unique())  # Determine the number of classes dynamically\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-15T11:44:08.308209Z","iopub.execute_input":"2023-12-15T11:44:08.308979Z","iopub.status.idle":"2023-12-15T12:36:22.792650Z","shell.execute_reply.started":"2023-12-15T11:44:08.308934Z","shell.execute_reply":"2023-12-15T12:36:22.791760Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/kaggle/input/ratemeter/sample_submission.csv\n/kaggle/input/ratemeter/train.csv\n/kaggle/input/ratemeter/test.csv\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_42/91623653.py:43: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df_subset['review_text'] = df_subset['review_text'].apply(preprocess_and_vectorize)\n","output_type":"stream"},{"name":"stdout","text":"preprocessing is done ......\nvectorization is done ......\n","output_type":"stream"}]},{"cell_type":"code","source":"# Build the LSTM model for multi-class classification\nembedding_dim = 100\nmodel = Sequential()\nmodel.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=X.shape[1]))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100))\nmodel.add(Dense(num_classes, activation='softmax'))  # Use softmax for multi-class classification\n","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:40:06.886740Z","iopub.execute_input":"2023-12-15T12:40:06.887635Z","iopub.status.idle":"2023-12-15T12:40:09.240348Z","shell.execute_reply.started":"2023-12-15T12:40:06.887599Z","shell.execute_reply":"2023-12-15T12:40:09.239556Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nepochs = 5\nbatch_size = 1024\nfrom keras.callbacks import LearningRateScheduler\n\ndef lr_scheduler(epoch, lr):\n    if epoch % 10 == 0 and epoch > 0:\n        return lr * 0.9\n    return lr\n\nlr_schedule = LearningRateScheduler(lr_scheduler)\n\nmodel.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[lr_schedule])\n\n# Evaluate the model on the test set\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(f'Test Set Loss: {loss}, Test Set Accuracy: {accuracy}')\n","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:40:23.238914Z","iopub.execute_input":"2023-12-15T12:40:23.239622Z","iopub.status.idle":"2023-12-15T12:57:06.415180Z","shell.execute_reply.started":"2023-12-15T12:40:23.239588Z","shell.execute_reply":"2023-12-15T12:57:06.414314Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Epoch 1/5\n375/375 [==============================] - 174s 452ms/step - loss: 1.2090 - accuracy: 0.4825 - val_loss: 1.0988 - val_accuracy: 0.5273 - lr: 0.0010\nEpoch 2/5\n375/375 [==============================] - 169s 450ms/step - loss: 1.1043 - accuracy: 0.5283 - val_loss: 1.3199 - val_accuracy: 0.4440 - lr: 0.0010\n375/375 [==============================] - 169s 450ms/step - loss: 1.0914 - accuracy: 0.5351 - val_loss: 1.0759 - val_accuracy: 0.5388 - lr: 0.0010\nEpoch 4/5\n375/375 [==============================] - 169s 450ms/step - loss: 1.0443 - accuracy: 0.5531 - val_loss: 1.0631 - val_accuracy: 0.5443 - lr: 0.0010\nEpoch 5/5\n375/375 [==============================] - 169s 451ms/step - loss: 1.0238 - accuracy: 0.5615 - val_loss: 1.0458 - val_accuracy: 0.5526 - lr: 0.0010\n3750/3750 [==============================] - 150s 40ms/step - loss: 1.0455 - accuracy: 0.5558\nTest Set Loss: 1.0455151796340942, Test Set Accuracy: 0.5557500123977661\n","output_type":"stream"}]},{"cell_type":"code","source":"max_len = 1726  # Adjust to the desired sequence length\n\ntest_df = pd.read_csv('/kaggle/input/ratemeter/test.csv')\ntest_df['review_text'] = test_df['review_text'].apply(preprocess_and_vectorize)\n\n# Tokenize and pad the test data\nX_test = tokenizer.texts_to_sequences(test_df['review_text'])\nX_test = pad_sequences(X_test, max_len)\n\n# Predict using the model\ny_test_pred_probs = model.predict(X_test)\n\n# Convert predicted probabilities to class predictions\ny_test_pred = np.argmax(y_test_pred_probs, axis=1)\n\n# Save predictions to a CSV file\npredictions_df = pd.DataFrame({'review_id': test_df['review_id'], 'rating': y_test_pred})\npredictions_df.to_csv('lstm_multiclass_predictions2.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-15T13:25:28.732417Z","iopub.execute_input":"2023-12-15T13:25:28.732787Z","iopub.status.idle":"2023-12-15T13:54:23.646010Z","shell.execute_reply.started":"2023-12-15T13:25:28.732760Z","shell.execute_reply":"2023-12-15T13:54:23.644940Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"8438/8438 [==============================] - 318s 38ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}